{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage of Face detection and Cropping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_and_crop_face(image, cascade_path='haarcascade_frontalface_default.xml'):\n",
    "    \"\"\"\n",
    "    Detects faces in an image using Haar cascades and returns the cropped face region.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input image (BGR format)\n",
    "    - cascade_path: Filename for the Haar cascade XML file.\n",
    "    \n",
    "    Returns:\n",
    "    - face_roi: Cropped face region if a face is detected; otherwise, returns None.\n",
    "    - faces: List of detected face bounding boxes.\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale for the face detector\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Load Haar cascade from OpenCV's data directory\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + cascade_path)\n",
    "    \n",
    "    # Detect faces: adjust scaleFactor and minNeighbors as needed\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        print(\"No faces detected.\")\n",
    "        return None, faces\n",
    "\n",
    "    # For demonstration, take the first detected face (or choose the largest face)\n",
    "    (x, y, w, h) = sorted(faces, key=lambda b: b[2] * b[3], reverse=True)[0]\n",
    "    face_roi = image[y:y+h, x:x+w]\n",
    "    face_resized = cv2.resize(face_roi, (128, 128))\n",
    "    # return face_resized, faces\n",
    "    return face_roi, faces\n",
    "\n",
    "def show_image(title, image, cmap=None):\n",
    "    \"\"\"\n",
    "    Displays an image using Matplotlib.\n",
    "    \n",
    "    Parameters:\n",
    "    - title: Title of the plot.\n",
    "    - image: Image array.\n",
    "    - cmap: Color map (if needed, e.g., 'gray' for grayscale images).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage in Jupyter Notebook:\n",
    "img_path = r\"VGG2_Dataset\\n000043\\0002_01.jpg\"  # Update with your image path\n",
    "image = cv2.imread(img_path)\n",
    "\n",
    "if image is None:\n",
    "    print(\"Image not found or unable to load.\")\n",
    "else:\n",
    "    face, faces = detect_and_crop_face(image)\n",
    "    print(f\"Detected {len(faces)} face(s) in the image.\")\n",
    "    print(f\"Detected face: {face}\")\n",
    "    print(f\"Detected face shape: {face.shape if face is not None else None}\")\n",
    "    \n",
    "    # Convert images from BGR to RGB for proper display in Matplotlib\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if face is not None:\n",
    "        face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "        # Draw rectangles on the original image for visualization\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(image_rgb, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the original image with detected face boxes\n",
    "        show_image(\"Face Detection\", image_rgb)\n",
    "        \n",
    "        # Display the cropped face\n",
    "        show_image(\"Detected Face\", face_rgb)\n",
    "    else:\n",
    "        print(\"No face detected in the image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image (update 'path/to/your/image.jpg' with your image path)\n",
    "image = cv2.imread(r\"VGG2_Dataset\\n000033\\0001_01.jpg\")\n",
    "\n",
    "# Check if image was loaded\n",
    "if image is None:\n",
    "    print(\"Image not found or unable to load.\")\n",
    "else:\n",
    "    # Print the original image dimensions\n",
    "    print(\"Original image shape:\", image.shape)\n",
    "\n",
    "    # Define the target size (width, height)\n",
    "    target_size = (200, 200)\n",
    "\n",
    "    # Resize the image to the target size\n",
    "    resized_image = cv2.resize(image, target_size)\n",
    "    print(\"Resized image shape:\", resized_image.shape)\n",
    "\n",
    "    # Convert images from BGR (OpenCV default) to RGB for correct display in Matplotlib\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    resized_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the original and resized images side by side\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Resized image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(resized_rgb)\n",
    "    plt.title(f\"Resized Image {target_size[0]}x{target_size[1]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca_2d_1 = PCA(n_components=2)\n",
    "inital_features_2d = pca_2d_1.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(inital_features_2d[:, 0], inital_features_2d[:, 1], \n",
    "            c='royalblue', edgecolor='k', alpha=0.6)\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=12)\n",
    "plt.ylabel(\"Principal Component 2\", fontsize=12)\n",
    "plt.title(\"2D PCA Projection of VGG2 Features\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image_path = r\"VGG2_Dataset\\n000043\\0002_01.jpg\"  # Update with your image path\n",
    "example_image = cv2.imread(example_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LBP features of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_lbp(image):\n",
    "    \"\"\"\n",
    "    Display the original image and its LBP representation.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input image in BGR or grayscale.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image.copy()\n",
    "\n",
    "    # Compute LBP\n",
    "    lbp = local_binary_pattern(gray, 8, 1, method=\"uniform\")\n",
    "\n",
    "    # Plot the original image and LBP image\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    ax[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for correct color display\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    ax[1].imshow(lbp, cmap=\"gray\")\n",
    "    ax[1].set_title(\"LBP Image\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "display_lbp(example_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HoG features of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "\n",
    "def display_hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), orientations=9):\n",
    "    \"\"\"\n",
    "    Display the original image and its HOG visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input image in BGR format.\n",
    "    - pixels_per_cell: Size (in pixels) of a cell.\n",
    "    - cells_per_block: Number of cells in each block.\n",
    "    - orientations: Number of gradient orientations.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compute HOG features and visualization\n",
    "    _, hog_image = hog(gray, \n",
    "                        orientations=orientations,\n",
    "                        pixels_per_cell=pixels_per_cell,\n",
    "                        cells_per_block=cells_per_block,\n",
    "                        block_norm='L2-Hys',\n",
    "                        visualize=True,\n",
    "                        feature_vector=False)\n",
    "\n",
    "    # Plot the original image and HOG image\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    ax[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for proper display\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    ax[1].imshow(hog_image, cmap=\"gray\")\n",
    "    ax[1].set_title(\"HOG Visualization\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "display_hog(example_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN features of an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pretrained ResNet-18\n",
    "def load_pretrained_model():\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Define preprocessing function\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to extract feature maps from an intermediate layer\n",
    "def extract_feature_maps(image, model, layer_name='layer1', device='cpu'):\n",
    "    \"\"\"\n",
    "    Extract feature maps from a specific layer of a CNN model.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input image (OpenCV format, BGR).\n",
    "    - model: Pretrained CNN model.\n",
    "    - layer_name: Name of the layer to extract features from.\n",
    "    - device: 'cuda' or 'cpu' for computation.\n",
    "    \n",
    "    Returns:\n",
    "    - feature_maps: Extracted feature maps as a NumPy array.\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert image to tensor\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Hook to capture feature maps\n",
    "    activation = {}\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        activation[layer_name] = output.detach()\n",
    "\n",
    "    # Register hook to capture output from the chosen layer\n",
    "    layer = dict(model.named_children())[layer_name]\n",
    "    hook = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Forward pass to extract feature maps\n",
    "    model(image)\n",
    "\n",
    "    # Remove hook\n",
    "    hook.remove()\n",
    "\n",
    "    # Convert feature maps to numpy array\n",
    "    feature_maps = activation[layer_name].cpu().numpy().squeeze()  # Shape: (C, H, W)\n",
    "    return feature_maps\n",
    "\n",
    "# Function to visualize feature maps\n",
    "def visualize_feature_maps(feature_maps, num_channels=8):\n",
    "    \"\"\"\n",
    "    Visualizes a few feature maps.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_maps: Extracted feature maps (NumPy array of shape CxHxW).\n",
    "    - num_channels: Number of feature maps to display.\n",
    "    \"\"\"\n",
    "    num_channels = min(num_channels, feature_maps.shape[0])  # Limit displayed channels\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i in range(num_channels):\n",
    "        plt.subplot(2, num_channels // 2, i + 1)\n",
    "        plt.imshow(feature_maps[i], cmap='viridis')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(\"CNN Feature Maps (Intermediate Layer)\")\n",
    "    plt.show()\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_pretrained_model().to(device)\n",
    "\n",
    "# Load an example image\n",
    "# example_img = cv2.imread('example.jpg')  # Replace with your image path\n",
    "example_img_1 = cv2.cvtColor(example_image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "# Extract feature maps from 'layer1'\n",
    "feature_maps = extract_feature_maps(example_img_1, model, layer_name='layer1', device=device)\n",
    "\n",
    "# Visualize feature maps\n",
    "visualize_feature_maps(feature_maps, num_channels=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal Number of Components for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'features' is your (N_samples, 8000) NumPy array\n",
    "# features = np.load('features.npy')  # or however you load your data\n",
    "\n",
    "# Initialize PCA without limiting number of components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_normalized)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.arange(1, len(cumulative_variance)+1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Explained Variance by Number of Principal Components\")\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.90, \"95% Threshold\", color = 'red', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Determine the number of components needed to reach 95% variance\n",
    "optimal_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(\"Optimal number of components for 95% variance:\", optimal_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data after reduction by PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce features to 2 components for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_pca)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.5, cmap='viridis')\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=12)\n",
    "plt.ylabel(\"Principal Component 2\", fontsize=12)\n",
    "plt.title(\"2D PCA Projection of reduced Features\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing results for Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_models import original_accuracy,optimized_accuracy\n",
    "from ML_models import rf_optimized\n",
    "from ML_models import rf_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Original RF', 'Optimized RF']\n",
    "accuracies = [original_accuracy, optimized_accuracy]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, accuracies, color=['#3498db', '#2ecc71'])\n",
    "plt.ylim(max(0.9, min(accuracies) - 0.05), min(1.0, max(accuracies) + 0.02))\n",
    "plt.xlabel('Random Forest Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest Accuracy Comparison')\n",
    "\n",
    "# Add accuracy values on top of bars\n",
    "for bar, accuracy in zip(bars, accuracies):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2 - 0.05,\n",
    "        bar.get_height() + 0.005,\n",
    "        f\"{accuracy:.4f}\",\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance of the optimized model\n",
    "feature_importances = rf_optimized.feature_importances_\n",
    "feature_names = range(X.shape[1])  # Assuming you have feature names\n",
    "\n",
    "# Sort features by importance\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "top_10_indices = indices[:10]  # Show top 10 features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(10), feature_importances[top_10_indices])\n",
    "plt.xticks(range(10), [f\"Feature {i}\" for i in top_10_indices])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 10 Feature Importances (Optimized Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
