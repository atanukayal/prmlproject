{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to Pytorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Store the original model's accuracy for comparison\n",
    "start_time = time.time()\n",
    "rf_original = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_original.fit(X_train, y_train)\n",
    "original_accuracy = rf_original.score(X_test, y_test)\n",
    "original_train_time = time.time() - start_time\n",
    "print(f\"Original Random Forest Accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"Training time: {original_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "print(\"\\nPerforming hyperparameter tuning with GridSearchCV...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    scoring='accuracy',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "tuning_time = time.time() - start_time\n",
    "\n",
    "# Best parameters and results\n",
    "print(f\"\\nBest parameters found: {grid_search.best_params_}\")\n",
    "print(f\"GridSearchCV took {tuning_time:.2f} seconds to complete\")\n",
    "\n",
    "# Create optimized model with best parameters\n",
    "rf_optimized = RandomForestClassifier(\n",
    "    **grid_search.best_params_,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_optimized.fit(X_train, y_train)\n",
    "optimized_accuracy = rf_optimized.score(X_test, y_test)\n",
    "optimized_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nOptimized Random Forest Accuracy: {optimized_accuracy:.4f}\")\n",
    "print(f\"Training time: {optimized_train_time:.2f} seconds\")\n",
    "print(f\"Accuracy improvement: {(optimized_accuracy - original_accuracy) * 100:.2f}%\")\n",
    "\n",
    "# Visualize results\n",
    "models = ['Original RF', 'Optimized RF']\n",
    "accuracies = [original_accuracy, optimized_accuracy]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, accuracies, color=['#3498db', '#2ecc71'])\n",
    "plt.ylim(max(0.9, min(accuracies) - 0.05), min(1.0, max(accuracies) + 0.02))\n",
    "plt.xlabel('Random Forest Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest Accuracy Comparison')\n",
    "\n",
    "# Add accuracy values on top of bars\n",
    "for bar, accuracy in zip(bars, accuracies):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2 - 0.05,\n",
    "        bar.get_height() + 0.005,\n",
    "        f\"{accuracy:.4f}\",\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance of the optimized model\n",
    "feature_importances = rf_optimized.feature_importances_\n",
    "feature_names = range(X.shape[1])  # Assuming you have feature names\n",
    "\n",
    "# Sort features by importance\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "top_10_indices = indices[:10]  # Show top 10 features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(10), feature_importances[top_10_indices])\n",
    "plt.xticks(range(10), [f\"Feature {i}\" for i in top_10_indices])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 10 Feature Importances (Optimized Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN :- K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for KNN\n",
    "print(\"\\nPerforming hyperparameter tuning for KNN...\")\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "tuning_time = time.time() - start_time\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"GridSearchCV took {tuning_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Visualize accuracy vs k value\n",
    "k_values = list(range(1, 21))\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, **{key: value for key, value in grid_search.best_params_.items() if key != 'n_neighbors'})\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, marker='o', linestyle='-', color='#3498db')\n",
    "plt.title('KNN Accuracy vs k Value')\n",
    "plt.xlabel('k (Number of Neighbors)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best KNN accuracy: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale')  # RBF kernel is often best for faces\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN :- Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "hidden_size = 64  # Number of neurons in hidden layers\n",
    "num_classes = len(label_encoder.classes_)  # Number of unique labels\n",
    "\n",
    "# Initialize the model\n",
    "model = ANNModel(input_size, hidden_size, num_classes)\n",
    "\n",
    "# ============================\n",
    "# 3. Train the Model\n",
    "# ============================\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50  \n",
    "batch_size = 32\n",
    "\n",
    "# Track accuracy\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        # Get mini-batch\n",
    "        X_batch = X_train_tensor[i:i+batch_size]\n",
    "        y_batch = y_train_tensor[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Training accuracy\n",
    "        train_outputs = model(X_train_tensor)\n",
    "        train_pred_classes = torch.argmax(train_outputs, axis=1)\n",
    "        train_accuracy = accuracy_score(y_train_tensor.numpy(), train_pred_classes.numpy())\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation accuracy\n",
    "        val_outputs = model(X_test_tensor)\n",
    "        val_pred_classes = torch.argmax(val_outputs, axis=1)\n",
    "        val_accuracy = accuracy_score(y_test_tensor.numpy(), val_pred_classes.numpy())\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print loss and accuracies for every epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, \"\n",
    "          f\"Train Accuracy: {train_accuracy * 100:.2f}%, \"\n",
    "          f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# ============================\n",
    "# 4. Evaluate the Model\n",
    "# ============================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_classes = torch.argmax(y_pred, axis=1).numpy()\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    print(f\"Final Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# ============================\n",
    "# 5. Save the Model\n",
    "# ============================\n",
    "torch.save(model.state_dict(), \"ann_model.pth\")\n",
    "print(\"Model saved as ann_model.pth\")\n",
    "\n",
    "# ============================\n",
    "# 6. Plot Training and Validation Accuracy\n",
    "# ============================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Store XGBoost results\n",
    "xgb_results = {}\n",
    "\n",
    "# ===================================\n",
    "# 2. Basic XGBoost Model\n",
    "# ===================================\n",
    "print(\"=== XGBoost Models ===\")\n",
    "\n",
    "# Basic XGBoost (default parameters)\n",
    "xgb_basic = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_basic.fit(X_train, y_train)\n",
    "y_pred = xgb_basic.predict(X_test)\n",
    "xgb_basic_acc = accuracy_score(y_test, y_pred)\n",
    "xgb_results['XGBoost (basic)'] = xgb_basic_acc\n",
    "print(f\"XGBoost (basic) Accuracy: {xgb_basic_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "xgb_improved = GridSearchCV(\n",
    "    XGBClassifier(random_state=42),\n",
    "    param_grid_xgb,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_improved.fit(X_train, y_train)\n",
    "y_pred = xgb_improved.predict(X_test)\n",
    "xgb_improved_acc = accuracy_score(y_test, y_pred)\n",
    "xgb_results['XGBoost (improved)'] = xgb_improved_acc\n",
    "print(f\"XGBoost (improved) Accuracy: {xgb_improved_acc:.4f}\")\n",
    "print(f\"Best XGBoost Parameters: {xgb_improved.best_params_}\")\n",
    "\n",
    "# ================================\n",
    "# 4. Plot XGBoost Results\n",
    "# ================================\n",
    "plt.figure(figsize=(8, 5))\n",
    "models = list(xgb_results.keys())\n",
    "accuracies = list(xgb_results.values())\n",
    "plt.bar(models, accuracies, color=['#66b3ff', '#ff9999'])\n",
    "plt.xlabel('XGBoost Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('XGBoost Model Accuracy Comparison')\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
